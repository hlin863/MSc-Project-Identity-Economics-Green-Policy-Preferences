{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy.stats import chisquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_synthetic_responses_path = 'C:\\\\Users\\\\haoch\\\\Documents\\\\COMP0190\\\\Data\\\\COMP0191-MSc-Project-Code\\\\Synthetic-Responses-JSON'\n",
    "base_data_path = 'C:\\\\Users\\\\haoch\\\\Documents\\\\COMP0190\\\\Data\\\\COMP0191-MSc-Project-Code\\\\Environmental-Views-Variables'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1_json_file = \"\\\\synthetic_responses_demo.json\"\n",
    "question_1_data_file = \"\\\\scenv_crlf\\\\Environmental Friendly Behaviour Probability Distribution Wave 10.json\"\n",
    "question_1_response_options = [\"don't do anything environmentally friendly\", 'do one or two things environmentally friendly', 'do some things environmentally friendly', 'do many things environmentally friendly', 'do everything environmentally friendly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_2_json_file = \"\\\\q2_synthetic_responses.json\"\n",
    "question_2_data_file = \"\\\\scenv_bccc\\\\Scenv Bccc Probability Distribution Wave 10.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_3_json_file = \"\\\\q3_synthetic_responses.json\"\n",
    "question_3_data_file = \"\\\\scenv_pmep\\\\Scenv Pmep Probability Distribution Wave 10.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_4_json_file = \"\\\\q4_synthetic_responses.json\"\n",
    "question_4_data_file = \"\\\\OpenVB\\\\Open VB Probability Distribution Wave 18.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_5_json_file = \"\\\\q5_synthetic_responses.json\"\n",
    "question_5_data_file = \"\\\\scenv_meds\\\\Scenv Meds Probability Distribution Wave 10.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_6_json_file = \"\\\\q6_synthetic_responses.json\"\n",
    "question_6_data_file = \"\\\\etariff\\\\etariff Probability Distribution Wave 10.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_7_json_file = \"\\\\q7_synthetic_responses.json\"\n",
    "question_7_data_file = \"\\\\grimyn\\\\grimyn Probability Distribution Wave 3.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_8_json_file = \"\\\\q8_synthetic_responses.json\"\n",
    "question_8_data_file = \"\\\\orga3\\\\orga3 Probability Distribution Wave 3.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_9_json_file = \"\\\\q9_synthetic_responses.json\"\n",
    "question_9_data_file = \"\\\\scenv_tlat\\\\Scenv Tlat Probability Distribution Wave 10.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_json_files = [question_1_json_file, question_2_json_file, question_3_json_file, question_4_json_file, question_5_json_file, question_6_json_file, question_7_json_file, question_8_json_file, question_9_json_file]\n",
    "questions_data_files = [question_1_data_file, question_2_data_file, question_3_data_file, question_4_data_file, question_5_data_file, question_6_data_file, question_7_data_file, question_8_data_file, question_9_data_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAF1 and Absolute Difference Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_round(v, total_responses):\n",
    "    \"\"\"\n",
    "    Rounds a value based on the total number of responses.\n",
    "    Parameters:\n",
    "    v (float): The value to be rounded.\n",
    "    total_responses (int): The total number of responses.\n",
    "    Returns:\n",
    "    int: The rounded value.\n",
    "    \"\"\"\n",
    "    product = v * total_responses\n",
    "    integer_part = math.floor(product)\n",
    "    decimal_part = product - integer_part\n",
    "    \n",
    "    if decimal_part > 0.5:\n",
    "        return math.ceil(product)\n",
    "    else:\n",
    "        return integer_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_maf1_and_absolute_difference(base_data_path, question_2_data_file, base_synthetic_responses_path, question_2_json_file, ordered_option):\n",
    "    \"\"\"\n",
    "    Calculates the MAF1 score and absolute differences between synthetic and actual data.\n",
    "    Parameters:\n",
    "    - base_data_path (str): The base path of the data file.\n",
    "    - question_2_data_file (str): The file name of the data file.\n",
    "    - base_synthetic_responses_path (str): The base path of the synthetic responses file.\n",
    "    - question_2_json_file (str): The file name of the synthetic responses file.\n",
    "    - ordered_option (list): The ordered list of categories.\n",
    "    Returns:\n",
    "    - absolute_differences (dict): A dictionary containing the absolute differences between synthetic and actual data.\n",
    "    - normalized_absolute_differences (dict): A dictionary containing the normalized absolute differences between synthetic and actual data.\n",
    "    - cosine_similarity_score (float): The cosine similarity score between synthetic and actual data.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    with open(base_data_path + question_2_data_file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(base_synthetic_responses_path + question_2_json_file) as f:\n",
    "        responses = json.load(f)\n",
    "\n",
    "    # Aggregate synthetic responses\n",
    "    synthetic_counts = Counter()\n",
    "    for response in responses:\n",
    "        synthetic_counts.update(response[\"Synthetic Responses\"])\n",
    "\n",
    "    # Convert counts to proportions\n",
    "    total_responses = sum(synthetic_counts.values())\n",
    "    synthetic_proportions = {k: v / total_responses for k, v in synthetic_counts.items()}\n",
    "\n",
    "    # convert the keys of synthetic_proportions to lowercase\n",
    "    synthetic_proportions = {k.lower(): v for k, v in synthetic_proportions.items()}\n",
    "    data = {k.lower(): v for k, v in data.items()}\n",
    "\n",
    "    # if the keys are \"neither agree nor disagree\", simplify the key to neither\n",
    "    if \"neither agree nor disagree\" in synthetic_proportions:\n",
    "        synthetic_proportions[\"neither\"] = synthetic_proportions[\"neither agree nor disagree\"]\n",
    "        del synthetic_proportions[\"neither agree nor disagree\"]\n",
    "\n",
    "    if \"neither agree nor disagree\" in data:\n",
    "        data[\"neither\"] = data[\"neither agree nor disagree\"]\n",
    "        del data[\"neither agree nor disagree\"]\n",
    "\n",
    "    # Ensure all keys in UKHLS data are present in synthetic data\n",
    "    for key in data.keys():\n",
    "        if key not in synthetic_proportions:\n",
    "            synthetic_proportions[key] = 0\n",
    "\n",
    "    # Compute the absolute differences\n",
    "    absolute_differences = {k: abs(synthetic_proportions[k] - data[k]) for k in data.keys()}\n",
    "\n",
    "    normalized_absolute_differences = {k: v / data[k] for k, v in absolute_differences.items()}\n",
    "\n",
    "    # multiply both synthetic_proportions and data by total_responses to get the counts in integer form\n",
    "    synthetic_proportions = {k: custom_round(v, total_responses) for k, v in synthetic_proportions.items()}\n",
    "    data = {k: custom_round(v, total_responses) for k, v in data.items()}\n",
    "\n",
    "    cosine_similarity_score = cosine_similarity([list(synthetic_proportions.values())], [list(data.values())])[0][0]  \n",
    "\n",
    "    print(data)\n",
    "\n",
    "    # sort the keys of synthetic proportions in the order of ordered option\n",
    "    synthetic_proportions = {k: synthetic_proportions[k] for k in ordered_option}\n",
    "    print(synthetic_proportions)\n",
    "\n",
    "    categories = ordered_option\n",
    "\n",
    "    # initialise a contingency table using the categories, synthetic_proportions and data\n",
    "    contingency_table = pd.DataFrame({\n",
    "        'Category': categories,\n",
    "        'Synthetic Proportions': [synthetic_proportions[category] for category in categories],\n",
    "        'Expected Data': [data[category] for category in categories]\n",
    "    })\n",
    "\n",
    "    contingency_table.set_index('Category', inplace=True)\n",
    "\n",
    "    # Perform the Chi-Squared test\n",
    "    chi2_stat, p_value = chisquare(contingency_table['Synthetic Proportions'], contingency_table['Expected Data'])\n",
    "\n",
    "    print(f\"Chi-Squared Statistic: {chi2_stat}\")\n",
    "    print(f\"P-Value: {p_value}\")\n",
    "\n",
    "    return absolute_differences, normalized_absolute_differences, cosine_similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_options = [\n",
    "    [\"don't do anything environmentally friendly\", 'do one or two things environmentally friendly', 'do some things environmentally friendly', 'do many things environmentally friendly', 'do everything environmentally friendly'],\n",
    "    ['strongly agree', 'tend to agree', 'neither', 'tend to disagree', 'strongly disagree'],\n",
    "    ['strongly agree', 'tend to agree', 'tend to disagree', 'strongly disagree', 'neither'],\n",
    "    ['strongly agree', 'agree', 'disagree', 'strongly disagree', 'already changed'],\n",
    "    ['strongly agree', 'tend to agree', 'neither', 'tend to disagree', 'strongly disagree'],\n",
    "    ['yes - already buy', 'yes - seriously considering', 'no', 'considered and rejected'],\n",
    "    ['yes', 'no'],\n",
    "    ['mentioned', 'not mentioned'],\n",
    "    ['strongly agree', 'tend to agree', 'neither', 'tend to disagree', 'strongly disagree']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"don't do anything environmentally friendly\": 6, 'do one or two things environmentally friendly': 36, 'do some things environmentally friendly': 40, 'do many things environmentally friendly': 16, 'do everything environmentally friendly': 2}\n",
      "{\"don't do anything environmentally friendly\": 4, 'do one or two things environmentally friendly': 2, 'do some things environmentally friendly': 52, 'do many things environmentally friendly': 42, 'do everything environmentally friendly': 0}\n",
      "Chi-Squared Statistic: 80.62777777777778\n",
      "P-Value: 1.2823205101362282e-16\n",
      "Question 1 Absolute Differences: {\"don't do anything environmentally friendly\": 0.01744607582255893, 'do one or two things environmentally friendly': 0.3365726450239489, 'do some things environmentally friendly': 0.11546692546097737, 'do many things environmentally friendly': 0.2576796168174561, 'do everything environmentally friendly': 0.019127821431925617}\n",
      "Question 1 Cosine Similarity Scores: 0.5199567597069386\n",
      "{'strongly agree': 9, 'tend to agree': 37, 'neither': 34, 'tend to disagree': 17, 'strongly disagree': 3}\n",
      "{'strongly agree': 3, 'tend to agree': 13, 'neither': 42, 'tend to disagree': 32, 'strongly disagree': 10}\n",
      "Chi-Squared Statistic: 51.01854795972443\n",
      "P-Value: 2.2123860269992281e-10\n",
      "Question 2 Absolute Differences: {'strongly agree': 0.06130000000000001, 'tend to agree': 0.2375, 'neither': 0.08089999999999997, 'tend to disagree': 0.1514, 'strongly disagree': 0.07490000000000001}\n",
      "Question 2 Cosine Similarity Scores: 0.679647258449488\n",
      "{'strongly agree': 7, 'tend to agree': 38, 'tend to disagree': 19, 'strongly disagree': 5, 'neither': 31}\n",
      "{'strongly agree': 47, 'tend to agree': 32, 'tend to disagree': 11, 'strongly disagree': 8, 'neither': 2}\n",
      "Chi-Squared Statistic: 261.81625030317736\n",
      "P-Value: 1.851804597514113e-55\n",
      "Question 3 Absolute Differences: {'strongly agree': 0.40198999999999996, 'tend to agree': 0.06420999999999999, 'tend to disagree': 0.08078999999999999, 'strongly disagree': 0.03468, 'neither': 0.29061}\n",
      "Question 3 Cosine Similarity Scores: 0.7445193128252923\n",
      "{'strongly agree': 5, 'agree': 45, 'disagree': 28, 'strongly disagree': 2, 'already changed': 20}\n",
      "{'strongly agree': 56, 'agree': 2, 'disagree': 31, 'strongly disagree': 9, 'already changed': 2}\n",
      "Chi-Squared Statistic: 602.3103174603175\n",
      "P-Value: 4.900116214806317e-129\n",
      "Question 4 Absolute Differences: {'strongly agree': 0.51, 'agree': 0.43, 'disagree': 0.02999999999999997, 'strongly disagree': 0.06999999999999999, 'already changed': 0.18000000000000002}\n",
      "Question 4 Cosine Similarity Scores: 0.4337773606479943\n",
      "{'strongly agree': 18, 'tend to agree': 41, 'neither': 30, 'tend to disagree': 9, 'strongly disagree': 2}\n",
      "{'strongly agree': 5, 'tend to agree': 13, 'neither': 30, 'tend to disagree': 38, 'strongly disagree': 14}\n",
      "Chi-Squared Statistic: 193.9552845528455\n",
      "P-Value: 7.486383588402787e-41\n",
      "Question 5 Absolute Differences: {'strongly agree': 0.13344749929623723, 'tend to agree': 0.2756801476337931, 'neither': 0.00016577523380562065, 'tend to disagree': 0.29279597134903507, 'strongly disagree': 0.11616590034718965}\n",
      "Question 5 Cosine Similarity Scores: 0.6452997861996634\n",
      "{'yes - already buy': 9, 'yes - seriously considering': 8, 'no': 81, 'considered and rejected': 2}\n",
      "{'yes - already buy': 15, 'yes - seriously considering': 57, 'no': 28, 'considered and rejected': 0}\n",
      "Chi-Squared Statistic: 340.804012345679\n",
      "P-Value: 1.4615382520136525e-73\n",
      "Question 6 Absolute Differences: {'yes - already buy': 0.062218202059560246, 'yes - seriously considering': 0.4914583913164486, 'no': 0.5334149735596994, 'considered and rejected': 0.02026161981630949}\n",
      "Question 6 Cosine Similarity Scores: 0.3651873495175622\n",
      "{'yes': 11, 'no': 89}\n",
      "{'yes': 69, 'no': 31}\n",
      "Chi-Squared Statistic: 343.6159346271706\n",
      "P-Value: 1.0408698614818738e-76\n",
      "Question 7 Absolute Differences: {'yes': 0.5773834814654237, 'no': 0.5773834814654237}\n",
      "Question 7 Cosine Similarity Scores: 0.5186087906771539\n",
      "{'mentioned': 2, 'not mentioned': 98}\n",
      "{'mentioned': 52, 'not mentioned': 48}\n",
      "Chi-Squared Statistic: 1275.5102040816328\n",
      "P-Value: 2.37266622862656e-279\n",
      "Question 8 Absolute Differences: {'mentioned': 0.5025230660042583, 'not mentioned': 0.5025230660042583}\n",
      "Question 8 Cosine Similarity Scores: 0.7484900414507708\n",
      "{'strongly agree': 2, 'tend to agree': 12, 'neither': 33, 'tend to disagree': 39, 'strongly disagree': 14}\n",
      "{'strongly agree': 0, 'tend to agree': 4, 'neither': 31, 'tend to disagree': 43, 'strongly disagree': 22}\n",
      "Chi-Squared Statistic: 12.436230436230435\n",
      "P-Value: 0.014385689526488353\n",
      "Question 9 Absolute Differences: {'strongly agree': 0.023180879684664958, 'tend to agree': 0.0769993117687543, 'neither': 0.015971344553588174, 'tend to disagree': 0.03798973909779141, 'strongly disagree': 0.07816179690921604}\n",
      "Question 9 Cosine Similarity Scores: 0.4889767875767537\n"
     ]
    }
   ],
   "source": [
    "question_index = 0\n",
    "\n",
    "sum_maf1 = 0\n",
    "sum_absolute_differences = Counter()\n",
    "\n",
    "overall_normalized_absolute_differences = []\n",
    "\n",
    "cosine_similarity_scores = []\n",
    "\n",
    "for question_json_file, data_json_file, ordered_option in zip(questions_json_files, questions_data_files, ordered_options):\n",
    "    absolute_differences, normalized_absolute_differences, cosine_similarity_score = calculate_maf1_and_absolute_difference(base_data_path, data_json_file, base_synthetic_responses_path, question_json_file, ordered_option)\n",
    "    print(f\"Question {question_index + 1} Absolute Differences: {absolute_differences}\")\n",
    "    # print(f\"Question {question_index + 1} Normalized Absolute Differences: {normalized_absolute_differences}\")\n",
    "    print(f\"Question {question_index + 1} Cosine Similarity Scores: {cosine_similarity_score}\")\n",
    "\n",
    "    # store the absolute differences in text files\n",
    "    with open(f\"question_{question_index + 1}_absolute_differences.txt\", \"w\") as f:\n",
    "        f.write(json.dumps(absolute_differences))\n",
    "\n",
    "    overall_normalized_absolute_differences.append(normalized_absolute_differences)\n",
    "\n",
    "    sum_absolute_differences.update(absolute_differences)\n",
    "\n",
    "    cosine_similarity_scores.append(cosine_similarity_score)\n",
    "\n",
    "    question_index += 1\n",
    "\n",
    "# write cosine similarity score to a text file\n",
    "with open(\"cosine_similarity_scores.text\", \"w\") as f:\n",
    "    f.write(json.dumps(cosine_similarity_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Absolute Difference: 0.3881148866061161\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total sum of absolute differences and the number of differences\n",
    "total_sum_absolute_differences = 0\n",
    "total_number_of_differences = 0\n",
    "\n",
    "for diff, count in sum_absolute_differences.items():\n",
    "    # Ensure that the difference is a number\n",
    "    try:\n",
    "        total_sum_absolute_differences += count\n",
    "        total_number_of_differences += 1\n",
    "    except (ValueError, TypeError) as e:\n",
    "        pass\n",
    "\n",
    "# Calculate the average of absolute differences\n",
    "if total_number_of_differences > 0:\n",
    "    average_absolute_difference = total_sum_absolute_differences / total_number_of_differences\n",
    "else:\n",
    "    average_absolute_difference = 0\n",
    "\n",
    "print(f\"Average Absolute Difference: {average_absolute_difference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"don't do anything environmentally friendly\": 0.3036948228882833, 'do one or two things environmentally friendly': 0.9439104477611939, 'do some things environmentally friendly': 0.28543259557344064, 'do many things environmentally friendly': 1.5874754098360655, 'do everything environmentally friendly': 1.0}\n",
      "{'strongly agree': 0.6714129244249727, 'tend to agree': 0.6462585034013605, 'neither': 0.23857269242111462, 'tend to disagree': 0.8979833926453143, 'strongly disagree': 2.9840637450199203}\n",
      "{'strongly agree': 5.910748419350095, 'tend to agree': 0.16712214674266673, 'tend to disagree': 0.42344986634519627, 'strongly disagree': 0.76522506619594, 'neither': 0.9356105727439554}\n",
      "{'strongly agree': 10.2, 'agree': 0.9555555555555555, 'disagree': 0.10714285714285703, 'strongly disagree': 3.4999999999999996, 'already changed': 0.9}\n",
      "{'strongly agree': 0.7274424552429668, 'tend to agree': 0.679550501156515, 'neither': 0.0005528896307114018, 'tend to disagree': 3.357596843615495, 'strongly disagree': 4.873937007874017}\n",
      "{'yes - already buy': 0.7087824984147113, 'yes - seriously considering': 6.257299787384833, 'no': 0.655772257578868, 'considered and rejected': 1.0}\n",
      "{'yes': 5.126987487969201, 'no': 0.6506583608159276}\n",
      "{'mentioned': 28.753502538071068, 'not mentioned': 0.5114618510158013}\n",
      "{'strongly agree': 1.0, 'tend to agree': 0.6581176470588235, 'neither': 0.04899616122840687, 'tend to disagree': 0.09691006304365175, 'strongly disagree': 0.5510630789589767}\n"
     ]
    }
   ],
   "source": [
    "for normalized_difference in overall_normalized_absolute_differences:\n",
    "    print(normalized_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_expected_mean(distribution, question_dictionary_mapping):\n",
    "    \"\"\"\n",
    "    Calculate the expected mean for a given probability distribution.\n",
    "\n",
    "    :param distribution: Dictionary where keys are response levels and values are their corresponding probabilities\n",
    "    :param question_dictionary_mapping: Dictionary mapping response levels to their corresponding numerical values\n",
    "    :return: Expected mean value of the distribution\n",
    "    \"\"\"\n",
    "    # try to convert every key in the question_dictionary_mapping to lowercase\n",
    "    question_dictionary_mapping = {v.lower(): k for k, v in question_dictionary_mapping.items()}\n",
    "\n",
    "    # print(question_dictionary_mapping)\n",
    "\n",
    "    expected_mean = sum(question_dictionary_mapping[response] * prob for response, prob in distribution.items())\n",
    "    return expected_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_maf1_and_absolute_difference_h1(base_data_path, question_2_data_file, question_2_json_file, question_dictionary_mapping):\n",
    "    # Load data\n",
    "    with open(base_data_path + question_2_data_file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(base_synthetic_responses_path + question_2_json_file) as f:\n",
    "        responses = json.load(f)\n",
    "\n",
    "    # convert the keys of synthetic_proportions to lowercase\n",
    "    synthetic_proportions = {k.lower(): v for k, v in responses.items()}\n",
    "    data = {k.lower(): v for k, v in data.items()}\n",
    "\n",
    "    total_responses = sum(data.values())\n",
    "\n",
    "    synthetic_proportions = {k: v / total_responses for k, v in synthetic_proportions.items()}\n",
    "    data = {k: v / total_responses for k, v in data.items()}\n",
    "\n",
    "    # if the keys are \"neither agree nor disagree\", simplify the key to neither\n",
    "    if \"neither agree nor disagree\" in synthetic_proportions:\n",
    "        synthetic_proportions[\"neither\"] = synthetic_proportions[\"neither agree nor disagree\"]\n",
    "        del synthetic_proportions[\"neither agree nor disagree\"]\n",
    "\n",
    "    if \"neither agree nor disagree\" in data:\n",
    "        data[\"neither\"] = data[\"neither agree nor disagree\"]\n",
    "        del data[\"neither agree nor disagree\"]\n",
    "\n",
    "    # Ensure all keys in UKHLS data are present in synthetic data\n",
    "    for key in data.keys():\n",
    "        if key not in synthetic_proportions:\n",
    "            synthetic_proportions[key] = 0\n",
    "\n",
    "    print(\"Synthetic Distribution: \", synthetic_proportions)\n",
    "    print(\"Data Distribution: \", data)\n",
    "\n",
    "    expected_synthetic_distribution_value = calculate_expected_mean(synthetic_proportions, question_dictionary_mapping)\n",
    "    expected_data_distribution_value = calculate_expected_mean(data, question_dictionary_mapping)\n",
    "\n",
    "    print(f\"Expected Synthetic Distribution Value: {expected_synthetic_distribution_value}\")\n",
    "    print(f\"Expected Data Distribution Value: {expected_data_distribution_value}\")\n",
    "\n",
    "    # Compute the absolute differences\n",
    "    absolute_differences = {k: abs(synthetic_proportions[k] - data[k]) for k in data.keys()}\n",
    "\n",
    "    # normalized_absolute_differences = {k: v / data[k] for k, v in absolute_differences.items()}\n",
    "    normalized_absolute_differences = 0\n",
    "\n",
    "    # multiply both synthetic_proportions and data by total_responses to get the counts in integer form\n",
    "    synthetic_proportions = {k: int(v * total_responses) for k, v in synthetic_proportions.items()}\n",
    "    data = {k: int(v * total_responses) for k, v in data.items()}\n",
    "\n",
    "    cosine_similarity_score = cosine_similarity([list(synthetic_proportions.values())], [list(data.values())])[0][0]  \n",
    "\n",
    "    return absolute_differences, normalized_absolute_differences, cosine_similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenv_crlf_dict = {\n",
    "    1: \"Don't do Anything Environmentally Friendly\",\n",
    "    2: \"Do One or Two Things Environmentally Friendly\",\n",
    "    3: \"Do Some Things Environmentally Friendly\",\n",
    "    4: \"Do Many Things Environmentally Friendly\",\n",
    "    5: \"Do Everything Environmentally Friendly\"\n",
    "} # dictionary to create mapping for question 1\n",
    "\n",
    "q1_human_distribution = {\n",
    "    6: \"Entirely Positive\",\n",
    "    5: \"More Positive than Negative\",\n",
    "    4: \"Neither\",\n",
    "    3: \"More Negative than Positive\",\n",
    "    2: \"Entirely Negative\",\n",
    "    1: \"Don't Know\"\n",
    "} # distribution for question 2\n",
    "\n",
    "scenv_bccc_dict = {\n",
    "    1: \"Strongly Agree\",\n",
    "    2: \"Tend to Agree\",\n",
    "    3: \"Neither\",\n",
    "    4: \"Tend to Disagree\",\n",
    "    5: \"Strongly Disagree\"\n",
    "} # dictionary to create mapping for question 3\n",
    "\n",
    "scenv_pmep_dict = {\n",
    "    1: \"Strongly Agree\",\n",
    "    2: \"Tend to Agree\",\n",
    "    3: \"Neither\",\n",
    "    4: \"Tend to Disagree\",\n",
    "    5: \"Strongly Disagree\"\n",
    "} # dictionary to create mapping for question 4\n",
    "\n",
    "br_openvb_distribution = {\n",
    "    5: \"Strongly Agree\",\n",
    "    4: \"Agree\",\n",
    "    3: \"Disagree\",\n",
    "    2: \"Strongly Disagree\",\n",
    "    1: \"Already Changed\" \n",
    "} # initialise a dictionary to store the attitudes about whether personal changes are needed to protect the environment for question 5\n",
    "\n",
    "scenv_meds_dict = {\n",
    "    1: \"Strongly Agree\",\n",
    "    2: \"Tend to Agree\",\n",
    "    3: \"Neither\",\n",
    "    4: \"Tend to Disagree\",\n",
    "    5: \"Strongly Disagree\"\n",
    "} # dictionary to create mapping for question 6\n",
    "\n",
    "etariff_dict = {\n",
    "    1: \"Yes - already buy\",\n",
    "    2: \"Yes - seriously considering\",\n",
    "    3: \"No\",\n",
    "    4: \"Considered and rejected\"\n",
    "} # dictionary to create mapping for question 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_synthetic_responses_path = 'C:\\\\Users\\\\haoch\\\\Documents\\\\COMP0190\\\\Data\\\\COMP0191-MSc-Project-Code\\\\Synthetic-Responses-JSON'\n",
    "q1_with_children_json_paths = [\"\\\\Hypothesis-1\\\\synthetic_responses_question_7_wave_10_with_children.json\"]\n",
    "q1_without_children_json_paths = [\"\\\\Hypothesis-1\\\\synthetic_responses_question_7_wave_10_without_children.json\"]\n",
    "question_dictionary_mappings = [etariff_dict]\n",
    "wave_numbers = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data JSON File:  \\Hypothesis-1\\synthetic_responses_question_7_wave_10_without_children.json\n",
      "Question JSON File:  \\Hypothesis-1\\synthetic_responses_question_7_wave_10_with_children.json\n",
      "Synthetic Distribution:  {'yes - already buy': 0.31, 'yes - seriously considering': 0.45, 'no': 0.23, 'considered and rejected': 0.01}\n",
      "Data Distribution:  {'yes - already buy': 0.24, 'yes - seriously considering': 0.5, 'no': 0.26, 'considered and rejected': 0.0}\n",
      "Expected Synthetic Distribution Value: 1.94\n",
      "Expected Data Distribution Value: 2.02\n",
      "Question 1 Absolute Differences With and Without Children: {'yes - already buy': 0.07, 'yes - seriously considering': 0.04999999999999999, 'no': 0.03, 'considered and rejected': 0.01}\n",
      "Question 1 Cosine Similarity Scores With and Without Children: 0.9889639934559099\n"
     ]
    }
   ],
   "source": [
    "question_index = 0\n",
    "\n",
    "sum_maf1 = 0\n",
    "sum_absolute_differences = Counter()\n",
    "\n",
    "overall_normalized_absolute_differences = []\n",
    "\n",
    "cosine_similarity_scores = []\n",
    "\n",
    "for question_json_file, data_json_file, question_dictionary_mapping in zip(q1_with_children_json_paths, q1_without_children_json_paths, question_dictionary_mappings):\n",
    "    print(\"Data JSON File: \", data_json_file)\n",
    "    print(\"Question JSON File: \", question_json_file)\n",
    "    \n",
    "    absolute_differences, normalized_absolute_differences, cosine_similarity_score = calculate_maf1_and_absolute_difference_h1(base_synthetic_responses_path, data_json_file, question_json_file, question_dictionary_mapping)\n",
    "    print(f\"Question {question_index + 1} Absolute Differences With and Without Children: {absolute_differences}\")\n",
    "    # print(f\"Question {question_index + 1} Normalized Absolute Differences: {normalized_absolute_differences}\")\n",
    "    print(f\"Question {question_index + 1} Cosine Similarity Scores With and Without Children: {cosine_similarity_score}\")\n",
    "\n",
    "    overall_normalized_absolute_differences.append(normalized_absolute_differences)\n",
    "\n",
    "    sum_absolute_differences.update(absolute_differences)\n",
    "\n",
    "    cosine_similarity_scores.append(cosine_similarity_score)\n",
    "\n",
    "    question_index += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
